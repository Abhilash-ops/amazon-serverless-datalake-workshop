AWSTemplateFormatVersion: '2010-09-09'
Description: A serverless datalake workshop.
Resources:
  IngestionBucket:
    Type: AWS::S3::Bucket
  ApacheLogs:
    Type: AWS::Logs::LogGroup
    Properties: 
      LogGroupName: !Sub /${AWS::StackName}/apache
      RetentionInDays: 1
  ApacheLogsKinesis:
    Type: AWS::KinesisFirehose::DeliveryStream
    Properties: 
      DeliveryStreamType: DirectPut
      ExtendedS3DestinationConfiguration:
        RoleARN: !GetAtt ApacheLogsServiceRole.Arn
        BucketARN: !GetAtt IngestionBucket.Arn
        BufferingHints:
          IntervalInSeconds: 60
          SizeInMBs: 3
        CloudWatchLoggingOptions:
          Enabled: False
        CompressionFormat: UNCOMPRESSED
        Prefix: weblogs/live/
        ProcessingConfiguration:
          Enabled: true
          Processors: 
          - Type: Lambda
            Parameters:
            - ParameterName: LambdaArn
              ParameterValue: !Sub ${TransformKinesis.Arn}
            - ParameterName: BufferSizeInMBs
              ParameterValue: 3
            - ParameterName: BufferIntervalInSeconds
              ParameterValue: 60
            
  CloudWatchLogsToKinesis:
    Type: AWS::Logs::SubscriptionFilter
    Properties: 
      DestinationArn: !Sub ${ApacheLogsKinesis.Arn}
      FilterPattern: ""
      LogGroupName: !Sub ${ApacheLogs}
      RoleArn: !Sub ${LogsToKinesisServiceRole.Arn}
  LogsToKinesisServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: !Sub logs.${AWS::Region}.amazonaws.com
            Action: sts:AssumeRole
  LogsToKinesisRolePolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub ${AWS::StackName}_logs_kineis_policy
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - 'firehose:*'
            Resource:
              - !Sub '${ApacheLogsKinesis.Arn}'
          - Effect: Allow
            Action:
              - 'iam:PassRole'
            Resource:
              - !Sub '${LogsToKinesisServiceRole.Arn}'
      Roles:
        - !Ref 'LogsToKinesisServiceRole'
  ApacheLogsServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: firehose.amazonaws.com
            Action: sts:AssumeRole
  ApacheLogsRolePolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub ${AWS::StackName}_weblog_delivery_policy
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - 's3:*'
            Resource:
              - !Sub '${IngestionBucket.Arn}/*'
              - !Sub '${IngestionBucket.Arn}'
          - Effect: Allow
            Action: 
                - 'lambda:InvokeFunction'
                - 'lambda:InvokeAsync'
            Resource:
              - !Sub '${TransformKinesis.Arn}'
      Roles:
        - !Ref 'ApacheLogsServiceRole'
  TransformKinesisFunctionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
        - arn:aws:iam::aws:policy/AmazonS3FullAccess
  TransformKinesis:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: index.handler
      Runtime: python2.7
      Description: ''
      MemorySize: 512
      Timeout: 60
      Role: !GetAtt TransformKinesisFunctionRole.Arn
      Code: 
        ZipFile: !Sub |
          import base64
          import json
          import gzip
          import StringIO
          import boto3

          def transformLogEvent(log_event):
              return log_event['message'] + '\n'
              
          def processRecords(records):
              for r in records:
                  data = base64.b64decode(r['data'])
                  striodata = StringIO.StringIO(data)
                  with gzip.GzipFile(fileobj=striodata, mode='r') as f:
                      data = json.loads(f.read())

                  recId = r['recordId']
                  """
                  CONTROL_MESSAGE are sent by CWL to check if the subscription is reachable.
                  They do not contain actual data.
                  """
                  if data['messageType'] == 'CONTROL_MESSAGE':
                      yield {
                          'result': 'Dropped',
                          'recordId': recId
                      }
                  elif data['messageType'] == 'DATA_MESSAGE':
                      data = ''.join([transformLogEvent(e) for e in data['logEvents']])
                      print data
                      data = base64.b64encode(data)
                      yield {
                          'data': data,
                          'result': 'Ok',
                          'recordId': recId
                      }
                  else:
                      yield {
                          'result': 'ProcessingFailed',
                          'recordId': recId
                      }


          def handler(event, context):
              records = list(processRecords(event['records']))
              
              return {"records": records}

  RedshiftServiceRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: redshift.amazonaws.com
            Action: sts:AssumeRole
  RedshiftRolePolicy:
    Type: AWS::IAM::ManagedPolicy
    Properties:
      ManagedPolicyName: !Sub ${AWS::StackName}_redshift_policy
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Action:
              - 's3:*'
            Resource:
              - !Sub '${IngestionBucket.Arn}/*'
              - !Sub '${IngestionBucket.Arn}'
          - Effect: Allow
            Action:
              - 'glue:CreateDatabase'
              - 'glue:DeleteDatabase'
              - 'glue:GetDatabase'
              - 'glue:GetDatabases'
              - 'glue:UpdateDatabase'
              - 'glue:CreateTable'
              - 'glue:DeleteTable'
              - 'glue:BatchDeleteTable'
              - 'glue:UpdateTable'
              - 'glue:GetTable'
              - 'glue:GetTables'
              - 'glue:BatchCreatePartition'
              - 'glue:CreatePartition'
              - 'glue:DeletePartition'
              - 'glue:BatchDeletePartition'
              - 'glue:UpdatePartition'
              - 'glue:GetPartition'
              - 'glue:GetPartition'
              - 'glue:BatchGetPartition'
            Resource: '*'
      Roles:
        - !Ref 'RedshiftServiceRole'
  LoadSampleDataFunction:
    Type: 'AWS::Lambda::Function'
    Properties:
      Handler: index.lambda_handler
      Runtime: python2.7
      Description: ''
      MemorySize: 512
      Timeout: 600
      Role: !GetAtt TransformKinesisFunctionRole.Arn
      Environment:
        Variables:
          BUCKET_NAME: !Ref IngestionBucket
          SOURCE_BUCKET_NAME: arc326-instructions
      Code: 
        ZipFile: !Sub |
          import boto3
          import random
          import string
          import uuid
          import httplib
          import urlparse
          import json
          import base64
          import hashlib
          import os
          import cfnresponse


          s3_client = boto3.client('s3')
          s3 = boto3.resource('s3')

          def lambda_handler(event, context):
              try:
                  return process_cfn(event, context)
              except Exception as e:
                  print("EXCEPTION", e)
                  print(e)
                  responseData = {}
                  responseData['Data'] = "Create Failed"
                  cfnresponse.send(event, {
                      'StackId': event['StackId'],
                      'RequestId': event['RequestId'],
                      'LogicalResourceId': event['LogicalResourceId']
                      }, cfnresponse.FAILED, responseData)

          def delete_files(event, content):
              bucket = os.environ['BUCKET_NAME']
              marker = ''
              maxKeys = 100

              isTruncated = True
              s3_bucket = s3_client.list_objects_v2(Bucket=bucket, MaxKeys=maxKeys)

              while isTruncated:
                  isTruncated = s3_bucket['IsTruncated']
                  
                  objects = []

                  for obj in s3_bucket['Contents'] :
                      objects.append({"Key": obj["Key"]})


                  response = s3_client.delete_objects(
                      Bucket=bucket,
                      Delete={
                          'Objects': objects,
                          'Quiet': True
                      }
                  )

                  if isTruncated:
                      marker = s3_bucket['NextContinuationToken']
                      s3_bucket = s3_client.list_objects_v2(Bucket=bucket, ContinuationToken=marker, MaxKeys=maxKeys)

              return "Success"



          def process_cfn(event, context):
              print("Received event: " + json.dumps(event, indent=2))
              
              bucket = os.environ['BUCKET_NAME']

              response = {
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'Status': 'IN_PROCESS',
              }

              if event['RequestType'] == 'Delete':
                  delete_files(event, context)
                  responseData = {}
                  responseData['Data'] = "Delete Succcessful"
                  return cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
                   
              copy_files(event, context)
              responseData = {}
              responseData['Data'] = "Create Succcessful"
              return cfnresponse.send(event, context, cfnresponse.SUCCESS, responseData)
          def copy_files(event, context):
              print("Received event: " + json.dumps(event, indent=2))
              
              stackName =  event['ResourceProperties']['StackName']

              bucket = os.environ['BUCKET_NAME']
              sourceBucket = os.environ['SOURCE_BUCKET_NAME']

              response = s3_client.copy_object(
                  Bucket=bucket,
                  CopySource={'Bucket': 'arc326-instructions', 'Key': 'sample-data/useractivity.csv'},
                  Key='raw/useractivity/useractivity.csv'
              )

              print('useractivity copy complete')

              response = s3_client.copy_object(
                  Bucket=bucket,
                  CopySource={'Bucket': 'arc326-instructions', 'Key': 'sample-data/zipcodedata.csv'},
                  Key='raw/zipcodes/zipcodedata.csv'
              )

              response = s3_client.copy_object(
                  Bucket=bucket,
                  CopySource={'Bucket': 'arc326-instructions', 'Key': 'sample-data/userprofile.csv'},
                  Key='raw/userprofile/userprofile.csv'
              )

              src = s3.Object(sourceBucket, 'instructions/instructions-template.html')
              html = src.get()['Body'].read().decode('utf-8') 

              html = html.replace('^ingestionbucket^', bucket)
              html = html.replace('^stackname^', stackName)

              destination = s3.Object(bucket, 'instructions/instructions.html')
              result = destination.put(Body=html, ACL='public-read', ContentDisposition='inline', ContentType='text/html')


              return "Success"
  LoadSampleData:
    Type: Custom::LoadSampleData
    DependsOn:
      - IngestionBucket
    Properties: 
      ServiceToken: !GetAtt LoadSampleDataFunction.Arn
      StackName: !Ref AWS::StackName

Outputs:
  RedshiftRole:
    Description: The role that redshift uses to access data
    Value: !GetAtt RedshiftServiceRole.Arn
  WorkshopInstructionsUrl:
    Description: Follow the link for the instructions for the serverless datalake workshop.
    Value: !Sub https://s3.${AWS::Region}.amazonaws.com/${IngestionBucket}/instructions/instructions.html